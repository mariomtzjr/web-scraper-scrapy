{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping con Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Web Scraping es una técnica para extraer información de la web de manera automática cuando no se tiene una API que la proporcione.\n",
    "\n",
    "### Ventajas\n",
    "- No se depende de una API\n",
    "- No hay limitaciones (tiempo, información)\n",
    "\n",
    "### Desventajas\n",
    "- Se depende de la estructura de la página a la cual se realiza el scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proceso\n",
    "1. Se obtiene una url semilla\n",
    "2. Se realizan las solicitudes (Request)\n",
    "3. Se obtiene una respuesta (Response)\n",
    "4. Se obtiene la información de los items (Populate Items)\n",
    "5. Ir a más URLs y se repite el proceso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tipos de Web Scraping\n",
    "\n",
    "- Una sola página web  \n",
    "Se emplea un Spyder (*scrapy.spiders.Spyder*)\n",
    "  \n",
    "- Varías páginas web  \n",
    "Se emplrea un Crawler (*scrapy.spiders.CrawlSpider*). Se compone del **crawling vertical** y **crawling horizontal**.\n",
    "\n",
    "### Crawling vertical\n",
    "Es el barrido de información que se hace para cada uno de los items en una página (se accede a la página que es propia del item).\n",
    "\n",
    "### Crawling horizontal\n",
    "Es el barrido de información que se realiza para múltiples páginas de un mismo dominio (paginación) en donde están distribuidos los items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.item import Field, Item\n",
    "from scrapy.spiders import Spider\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.loader import ItemLoader\n",
    "\n",
    "\n",
    "class Pregunta(Item):\n",
    "    \"\"\"Clase que define los Items\n",
    "    id: corresponde al id secuencial de las preguntas\n",
    "    pregunta: corresponde a la pregunta de StackOverflow\n",
    "    \"\"\"\n",
    "    id = Field()\n",
    "    pregunta = Field()\n",
    "\n",
    "    \n",
    "class StackOverflowSpider(Spider):\n",
    "    name = \"Spider_01\"  # Nombre del Spider\n",
    "    start_urls = ['https://es.stackoverflow.com/questions']  # url semilla\n",
    "\n",
    "    def parse(seld, response):\n",
    "        selector = Selector(response)\n",
    "        # // -> Indica la raíz del arbol (padre)\n",
    "        # //*[@id=\"questions\"] -> Div con todas las preguntas\n",
    "        preguntas = selector.xpath('//div[@id=\"questions\"]/div')\n",
    "        # Se itera sobre las preguntas\n",
    "        for n, pregunta in enumerate(preguntas):\n",
    "            # Creamos el Item\n",
    "            i = ItemLoader(Pregunta(), pregunta)\n",
    "            # //*[@id=\"question-summary-189320\"]/div[2]/h3/a\n",
    "            i.add_xpath('pregunta', './/h3/a[@class=\"question-hyperlink\"]/text()')\n",
    "            i.add_value('id', n)\n",
    "\n",
    "            # retornamos el item\n",
    "            yield i.load_item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scrapy runspider spider.py -o stack_preguntas.csv -t csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.item import Field, Item\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.loader import ItemLoader\n",
    "from scrapy.loader.processors import MapCompose\n",
    "\n",
    "class CasaItem(Item):\n",
    "    tipo = Field()\n",
    "    capacidad = Field()\n",
    "    precio = Field()\n",
    "\n",
    "\n",
    "class CasaCrawler(CrawlSpider):\n",
    "    name = \"Crawler_01\"\n",
    "    start_urls = ['https://es-l.airbnb.com/s/homes?refinement_paths[]=homes/section/NEARBY_LISTINGS']\n",
    "    allowed_domains = ['airbnb.com']  # Lista de dominios permitidos\n",
    "    \n",
    "    # Restricciones que debe acatar el crawler\n",
    "    rules = (\n",
    "        Rule(LinkExtractor(allow=r'items_offset=')),  # Puede ingresar a las páginas que tienen en la url la expresión regular\n",
    "        Rule(LinkExtractor(allow=r'/rooms'), callback='parse_items'),\n",
    "    )\n",
    "    \n",
    "    # Función que es llamada cuando se cumpla la regla\n",
    "    def parse_items(self, response):\n",
    "        item = ItemLoader(CasaItem(), response)\n",
    "        item.add_xpath('tipo', '//*[@id=\"site-content\"]/div/div[4]/div/div/div[1]/div[2]/div/div/div/section/div/div[1]/div[2]/div[1]/text()')\n",
    "        item.add_xpath('capacidad', '//*[@id=\"site-content\"]/div/div[4]/div/div/div[1]/div[1]/div/div/div/div/section/div/div/div/div[1]/div[2]/span[1]/text()')\n",
    "        item.add_xpath('precio', '//*[@id=\"site-content\"]/div/div[4]/div/div/div[3]/div/div/div[1]/div/div/div/div/div[1]/div[1]/div/div/span/span[1][@class=\"_pgfqnw\"]/text()')\n",
    "        \n",
    "        yield item.load_item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
